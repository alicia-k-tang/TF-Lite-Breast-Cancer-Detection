{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Lite Breast Cancer Detection Week 13: Bring It All Together\n",
    "### Yinda Chen and Alice Tang\n",
    "\n",
    "For our final assignment, we will be including all of the most important code to represent an end to end modeling project, including only the most crucial parts of our process. \n",
    "\n",
    "Let's get started, shall we?\n",
    "\n",
    "To preface, the dataset can be found here: https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 13:54:59.157244: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-07 13:54:59.157284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-07 13:54:59.157730: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 13:54:59.160713: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 13:54:59.810172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "import uuid\n",
    "import shutil\n",
    "import random\n",
    "import glob as gb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from scipy.special import gamma\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv data\n",
    "\n",
    "dicom_df = pd.read_csv(\"../csv/dicom_info.csv\")\n",
    "mass_train = pd.read_csv(\"../csv/mass_case_description_train_set.csv\")\n",
    "mass_test  = pd.read_csv(\"../csv/mass_case_description_test_set.csv\")\n",
    "calc_train = pd.read_csv(\"../csv/calc_case_description_train_set.csv\")\n",
    "calc_test  = pd.read_csv(\"../csv/calc_case_description_test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing our image file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROI Mask Images paths:\n",
      "../jpeg/1.3.6.1.4.1.9590.100.1.2.153339052913121382622526066491844156138/2-270.jpg\n",
      "\n",
      "Cropped Images paths:\n",
      "../jpeg/1.3.6.1.4.1.9590.100.1.2.129308726812851964007517874181459556304/1-172.jpg\n",
      "\n",
      "Full mammo Images paths:\n",
      "../jpeg/1.3.6.1.4.1.9590.100.1.2.248386742010678582309005372213277814849/1-249.jpg\n"
     ]
    }
   ],
   "source": [
    "# Replace the path of images\n",
    "\n",
    "def replace_path(sample, old_path, new_path):\n",
    "    return sample.replace(old_path, new_path, regex=True)\n",
    "\n",
    "cropped_images = dicom_df[dicom_df.SeriesDescription==\"cropped images\"].image_path\n",
    "full_mammogram = dicom_df[dicom_df.SeriesDescription==\"full mammogram images\"].image_path\n",
    "roi_mask = dicom_df[dicom_df.SeriesDescription==\"ROI mask images\"].image_path\n",
    "\n",
    "correct_dir = \"../jpeg\"\n",
    "\n",
    "full_mammogram = replace_path(full_mammogram, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "cropped_images = replace_path(cropped_images, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "roi_mask = replace_path(roi_mask, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('\\nROI Mask Images paths:')\n",
    "print(roi_mask.iloc[0])\n",
    "print('\\nCropped Images paths:')\n",
    "print(cropped_images.iloc[0])\n",
    "print('\\nFull mammo Images paths:')\n",
    "print(full_mammogram.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file name list for every kind of images\n",
    "\n",
    "def get_image_file_name(data, new_dict):\n",
    "\n",
    "    for dicom in data:\n",
    "        key = dicom.split('/')[2]\n",
    "        new_dict[key] = dicom\n",
    "    print(f\"the length of dataset ==> {len(new_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of dataset ==> 2857\n",
      "the length of dataset ==> 3567\n",
      "the length of dataset ==> 3247\n"
     ]
    }
   ],
   "source": [
    "cropped_images_dict = dict()\n",
    "full_mammo_dict = dict()\n",
    "roi_img_dict = dict()\n",
    "\n",
    "get_image_file_name(full_mammogram, full_mammo_dict)\n",
    "get_image_file_name(cropped_images, cropped_images_dict)\n",
    "get_image_file_name(roi_mask, roi_img_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we must fix the image paths in our csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the image path in the csv\n",
    "\n",
    "def fix_image_path(data):\n",
    "    \"\"\"Correct dicom paths to correct image paths.\"\"\"\n",
    "    for indx, image in enumerate(data.values):\n",
    "\n",
    "        img_name = image[11].split('/')[2]\n",
    "\n",
    "        if img_name in full_mammo_dict:\n",
    "            data.iloc[indx, 11] = full_mammo_dict[img_name]\n",
    "        else:\n",
    "            data.iloc[indx, 11] = None\n",
    "        \n",
    "        img_name = image[12].split('/')[2]\n",
    "        if img_name in cropped_images_dict:\n",
    "            data.iloc[indx, 12] = cropped_images_dict[img_name]\n",
    "        else:\n",
    "            data.iloc[indx, 11] = None\n",
    "\n",
    "        img_name = image[13].split('/')[2]\n",
    "        if img_name in roi_img_dict:\n",
    "            data.iloc[indx, 13] = roi_img_dict[img_name]\n",
    "\n",
    "        else:\n",
    "            data.iloc[indx, 13] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing it for all of our csv dataset files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(mass_train)\n",
    "fix_image_path(mass_test)\n",
    "fix_image_path(calc_train)\n",
    "fix_image_path(calc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we must rename the name of our columns in the csv file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the name of columns in csv\n",
    "mass_train = mass_train.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                        'image view': 'image_view',\n",
    "                                        'abnormality id': 'abnormality_id',\n",
    "                                        'abnormality type': 'abnormality_type',\n",
    "                                        'mass shape': 'mass_shape',\n",
    "                                        'mass margins': 'mass_margins',\n",
    "                                        'image file path': 'image_file_path',\n",
    "                                        'cropped image file path': 'cropped_image_file_path',\n",
    "                                        'ROI mask file path': 'ROI_mask_file_path'})\n",
    "\n",
    "mass_test = mass_test.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                      'image view': 'image_view',\n",
    "                                      'abnormality id': 'abnormality_id',\n",
    "                                      'abnormality type': 'abnormality_type',\n",
    "                                      'mass shape': 'mass_shape',\n",
    "                                      'mass margins': 'mass_margins',\n",
    "                                      'image file path': 'image_file_path',\n",
    "                                      'cropped image file path': 'cropped_image_file_path',\n",
    "                                      'ROI mask file path': 'ROI_mask_file_path'})\n",
    "\n",
    "calc_train = calc_train.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                        'image view': 'image_view',\n",
    "                                        'abnormality id': 'abnormality_id',\n",
    "                                        'abnormality type': 'abnormality_type',\n",
    "                                        'mass shape': 'mass_shape',\n",
    "                                        'mass margins': 'mass_margins',\n",
    "                                        'image file path': 'image_file_path',\n",
    "                                        'cropped image file path': 'cropped_image_file_path',\n",
    "                                        'ROI mask file path': 'ROI_mask_file_path'})\n",
    "\n",
    "calc_test = calc_test.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                      'image view': 'image_view',\n",
    "                                      'abnormality id': 'abnormality_id',\n",
    "                                      'abnormality type': 'abnormality_type',\n",
    "                                      'mass shape': 'mass_shape',\n",
    "                                      'mass margins': 'mass_margins',\n",
    "                                      'image file path': 'image_file_path',\n",
    "                                      'cropped image file path': 'cropped_image_file_path',\n",
    "                                      'ROI mask file path': 'ROI_mask_file_path'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MALIGNANT', 'BENIGN', 'BENIGN_WITHOUT_CALLBACK'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_train.pathology.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the dataset into one for training\n",
    "\n",
    "full_dataset = pd.concat([mass_train, mass_test, calc_train, calc_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of classification\n",
    "\n",
    "class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14527/1650799513.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  full_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n"
     ]
    }
   ],
   "source": [
    "target_size = (224, 224, 3)\n",
    "\n",
    "# Apply class mapper to pathology column\n",
    "full_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n",
    "\n",
    "full_images = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"image_file_path\"].tolist())\n",
    "full_labels = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3284"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine the different counts of each label, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign images: 1930\n",
      "Malignant images: 1354\n"
     ]
    }
   ],
   "source": [
    "# If full_labels is a NumPy array, convert it to a Pandas series\n",
    "full_labels_series = pd.Series(full_labels)\n",
    "\n",
    "# Count the occurrences of each class\n",
    "label_counts = full_labels_series.value_counts()\n",
    "\n",
    "# Assuming 0 = benign and 1 = malignant\n",
    "benign_count = label_counts.get(0, 0)\n",
    "malignant_count = label_counts.get(1, 0)\n",
    "\n",
    "print(f\"Benign images: {benign_count}\")\n",
    "print(f\"Malignant images: {malignant_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "0    2111\n",
      "1    1457\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(full_dataset['labels'].unique())\n",
    "class_names = ['Benign', 'Malignant']\n",
    "\n",
    "# Check the distribution of labels\n",
    "label_counts = full_dataset['labels'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we must define our data augmentation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for data augmentation\n",
    "def augment_image(image):\n",
    "    # Apply data augmentation using tf.image functions\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "#     image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    return image\n",
    "\n",
    "# Function to resize image to (224, 224, 3)\n",
    "def resize_image(image_tensor):\n",
    "    return tf.image.resize(image_tensor, [224, 224])\n",
    "\n",
    "# Function to balance classes by augmenting images\n",
    "def copy_images_with_unique_filenames(images, labels, source, destination, target_count=None):\n",
    "    \"\"\"\n",
    "    Copy images from source to destination in subfolders '0' and '1',\n",
    "    ensuring unique filenames and applying data augmentation and balancing.\n",
    "    \"\"\"\n",
    "    benign_images = 0\n",
    "    malignant_images = 0\n",
    "    skipped_images = []\n",
    "\n",
    "    # Create the destination subfolders '0' and '1'\n",
    "    category_dest_dir_zero = os.path.join(destination, '0')\n",
    "    os.makedirs(category_dest_dir_zero, exist_ok=True)\n",
    "\n",
    "    category_dest_dir_one = os.path.join(destination, '1')\n",
    "    os.makedirs(category_dest_dir_one, exist_ok=True)\n",
    "\n",
    "    benign_images_list = []\n",
    "    malignant_images_list = []\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "#         img_name = data_frame.REFNUM[i]\n",
    "#         abs_path = os.path.join(source, img_name + '.pgm')\n",
    "\n",
    "        if os.path.exists(image):\n",
    "            try:\n",
    "                # Generate a unique filename\n",
    "                filename = os.path.basename(image)\n",
    "                unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n",
    "        \n",
    "                # Open the image using PIL\n",
    "                with Image.open(image) as img:\n",
    "                    # Convert the image to RGB mode (for saving as JPEG)\n",
    "                    img = img.convert('RGB')\n",
    "                    # Augment the image (convert it to a Tensor first)\n",
    "                    img_tensor = tf.convert_to_tensor(img)\n",
    "                    # Resize the image to (224, 224, 3)\n",
    "                    resized_img_tensor = resize_image(img_tensor)\n",
    "                    augmented_image_tensor = augment_image(resized_img_tensor)\n",
    "                    # Convert Tensor back to PIL image for saving\n",
    "                    augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n",
    "\n",
    "                    if label == 0:\n",
    "                        benign_images_list.append(unique_filename)\n",
    "                        dest_path = os.path.join(category_dest_dir_zero, unique_filename)\n",
    "#                         augmented_image.save(dest_path, 'JPEG')\n",
    "                        augmented_image.save(dest_path, 'JPEG')\n",
    "                        benign_images += 1\n",
    "\n",
    "                    elif label == 1:\n",
    "                        malignant_images_list.append(unique_filename)\n",
    "                        dest_path = os.path.join(category_dest_dir_one, unique_filename)\n",
    "#                         augmented_image.save(dest_path, 'JPEG')\n",
    "                        augmented_image.save(dest_path, 'JPEG')\n",
    "                        malignant_images += 1\n",
    "                        \n",
    "#                 del img, img_tensor, resized_img_tensor, augmented_image_tensor, augmented_image\n",
    "#                 gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying image {image}: {e}\")\n",
    "                skipped_images.append(image)\n",
    "        else:\n",
    "            print(f\"Image not found: {image}\")\n",
    "            skipped_images.append(image)\n",
    "\n",
    "    # If balancing is needed, duplicate/augment images from the smaller class\n",
    "    benign_count = len(benign_images_list)\n",
    "    malignant_count = len(malignant_images_list)\n",
    "\n",
    "    if benign_count < malignant_count:\n",
    "#         augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count - benign_count)\n",
    "        augment_and_save_images(benign_images_list, category_dest_dir_zero, malignant_count - benign_count)\n",
    "\n",
    "    elif malignant_count < benign_count:\n",
    "        augment_and_save_images(malignant_images_list, category_dest_dir_one, benign_count - malignant_count)\n",
    "\n",
    "    augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count)\n",
    "    augment_and_save_images(malignant_images_list, category_dest_dir_one, target_count)\n",
    "\n",
    "    print(f\"\\nCopying complete.\")\n",
    "    print(f\"Benign images copied (label 0): {benign_images}\")\n",
    "    print(f\"Benign count (label 0): {benign_count}\")\n",
    "    print(f\"Malignant images copied (label 1): {malignant_images}\")\n",
    "    print(f\"Malignant count (label 1): {malignant_count}\")\n",
    "    print(f\"Total skipped images: {len(skipped_images)}\")\n",
    "    if skipped_images:\n",
    "        print(\"Skipped images:\")\n",
    "        for img in skipped_images:\n",
    "            print(img)\n",
    "            \n",
    "\n",
    "# Function to augment and save images to balance the dataset\n",
    "def augment_and_save_images(images_list, destination_dir, num_augments):\n",
    "    \"\"\"\n",
    "    Augment and save images to balance the dataset.\n",
    "    \"\"\"\n",
    "    for i in range(num_augments):\n",
    "        img_name = random.choice(images_list)\n",
    "        abs_path = os.path.join(destination_dir, img_name)\n",
    "\n",
    "        try:\n",
    "            with Image.open(abs_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                # Augment the image\n",
    "                img_tensor = tf.convert_to_tensor(img)\n",
    "                # Resize the image\n",
    "#                 resized_img_tensor = resize_image(img_tensor)\n",
    "                augmented_image_tensor = augment_image(img_tensor)\n",
    "                # Convert Tensor back to PIL image for saving\n",
    "                augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n",
    "                # Remove the original extension from img_name 1-285.jpg --> 1-285\n",
    "                img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "                # Save augmented image with a unique name\n",
    "                augmented_image.save(os.path.join(destination_dir, img_name_without_ext + f'_aug{i}.jpg'), 'JPEG')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error augmenting image {abs_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"../jpeg\"\n",
    "destination_dir = \"../working/merged_images\"\n",
    "\n",
    "# target_count=0 meaning no Augmentation, There's just Data-Balance\n",
    "target_count = (len(full_labels) * 3) - len(full_labels)\n",
    "copy_images_with_unique_filenames(full_images, full_labels, source_dir, destination_dir, target_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the dir for the benign images and malignant images in the past weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in class 0: 8498\n",
      "Number of images in class 1: 8498\n"
     ]
    }
   ],
   "source": [
    "# Check the number of images in each class folder after merging\n",
    "zero_class_count = len(os.listdir(\"../working/merged_images/0\"))\n",
    "one_class_count  = len(os.listdir(\"../working/merged_images/1\"))\n",
    "\n",
    "print(f\"Number of images in class 0: {zero_class_count}\")\n",
    "print(f\"Number of images in class 1: {one_class_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16996 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 19:20:54.707965: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.720918: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.720945: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.722586: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.722606: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.722616: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.817933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.817966: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.817970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-09 19:20:54.817993: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 19:20:54.818004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-11-09 19:20:55.052016: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples:      981     batches(13) ==> 12753\n",
      "Validation samples: 261       batches(13) ==> 3393\n",
      "Test samples:       66      batches(13) ==> 858\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../working/merged_images'  # Update with the dataset path\n",
    "\n",
    "# Create a dataset for the entire data to use for split\n",
    "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(224, 224),\n",
    "    seed=50,\n",
    "    shuffle=True,\n",
    "    batch_size=13\n",
    ")\n",
    "# Calculate the total number of samples\n",
    "total_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "\n",
    "train_size = int(0.75 * total_samples)                 # 70% for training\n",
    "val_size   = int(0.2 * total_samples)                # 20% for validation\n",
    "test_size = total_samples - train_size - val_size     # 10% for testing\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset       = full_dataset.take(train_size)\n",
    "validation_dataset  = full_dataset.skip(train_size).take(val_size)\n",
    "test_dataset        = full_dataset.skip(train_size + val_size)\n",
    "\n",
    "train_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Train samples:      {train_size}     batches(13) ==> {train_size*13}\")\n",
    "print(f\"Validation samples: {val_size}       batches(13) ==> {val_size*13}\")\n",
    "print(f\"Test samples:       {test_size}      batches(13) ==> {test_size*13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we conduct our data improvements and enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MammogramPreProcessor:\n",
    "    def __init__(self, target_size=(224, 224)):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    # Function 1\n",
    "    @tf.function\n",
    "    def remove_background_tf(self, image):\n",
    "        \"\"\"\n",
    "        TensorFlow implementation for background removal.\n",
    "        \"\"\"\n",
    "        # Convert to grayscale if it's a 3-channel image\n",
    "        if tf.shape(image)[-1] == 3:\n",
    "            image = tf.image.rgb_to_grayscale(image)\n",
    "        \n",
    "        # Create a binary mask\n",
    "        threshold = tf.cast(5, dtype=tf.float32)\n",
    "        binary_mask = tf.cast(image > threshold, tf.float32)\n",
    "        \n",
    "        # Apply the mask\n",
    "        return image * binary_mask\n",
    "\n",
    "    # Function 2\n",
    "    @tf.function\n",
    "    def apply_clahe_tf(self, image):\n",
    "        \"\"\"\n",
    "        TensorFlow implementation for CLAHE enhancement.\n",
    "        \"\"\"\n",
    "        # Normalize to the range 0-255\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image - tf.reduce_min(image)) / (tf.reduce_max(image) - tf.reduce_min(image)) * 255\n",
    "        return image\n",
    "\n",
    "    # Function 3\n",
    "    @tf.function\n",
    "    def normalize_tf(self, image):\n",
    "        \"\"\"\n",
    "        Normalize the image.\n",
    "        \"\"\"\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        mean = tf.reduce_mean(image)\n",
    "        std = tf.math.reduce_std(image)\n",
    "        return (image - mean) / (std + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create a complete preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    processor = MammogramPreProcessor(target_size)\n",
    "    \n",
    "    def preprocess_function(images, labels):\n",
    "        # Process each image in the batch\n",
    "        def process_single_image(image):\n",
    "            # Remove background\n",
    "            image = processor.remove_background_tf(image)\n",
    "            \n",
    "            # Apply CLAHE enhancement\n",
    "            image = processor.apply_clahe_tf(image)\n",
    "            \n",
    "            # Normalize the image\n",
    "            image = processor.normalize_tf(image)\n",
    "            \n",
    "            # Ensure correct size\n",
    "            image = tf.image.resize(image, target_size)\n",
    "            \n",
    "            # Ensure the correct number of channels (if 3 channels are needed)\n",
    "            image = tf.tile(image, [1, 1, 3])\n",
    "            \n",
    "            return image\n",
    "        \n",
    "        # Process the entire batch\n",
    "        processed_images = tf.map_fn(process_single_image, images)\n",
    "        return processed_images, labels\n",
    "\n",
    "    return preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(full_dataset, batch_size=13):\n",
    "    \"\"\"\n",
    "    prepare for the dataset and preprocessing.\n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    # create the preprocess pipeline\n",
    "    preprocess_fn = create_preprocessing_pipeline(target_size=(224, 224))\n",
    "    \n",
    "    # apply the preprocess\n",
    "    processed_dataset = full_dataset.map(preprocess_fn, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # improve the performance\n",
    "    processed_dataset = processed_dataset.cache()\n",
    "    processed_dataset = processed_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get to modeling with our final model we chose- EfficientNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "\n",
    "def model(dropout, trainable_layers):\n",
    "    base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Calculate the index to start unfreezing layers\n",
    "    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - trainable_layers / 100.0)))\n",
    "\n",
    "    # Unfreeze layers from the calculated index onwards\n",
    "    for layer in base_model.layers[from_index:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Add custom layers on top (Upper-Layers)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 19:44:16.431008: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981/981 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.6878\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82582, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xpert/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981/981 [==============================] - 49s 43ms/step - loss: 0.6918 - accuracy: 0.6878 - val_loss: 0.4172 - val_accuracy: 0.8258 - lr: 1.0000e-04\n",
      "Epoch 2/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.4488 - accuracy: 0.8035\n",
      "Epoch 2: val_accuracy improved from 0.82582 to 0.86325, saving model to best_model.h5\n",
      "981/981 [==============================] - 41s 42ms/step - loss: 0.4488 - accuracy: 0.8035 - val_loss: 0.3231 - val_accuracy: 0.8632 - lr: 1.0000e-04\n",
      "Epoch 3/25\n",
      "980/981 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.8589\n",
      "Epoch 3: val_accuracy improved from 0.86325 to 0.91306, saving model to best_model.h5\n",
      "981/981 [==============================] - 41s 42ms/step - loss: 0.3324 - accuracy: 0.8590 - val_loss: 0.2273 - val_accuracy: 0.9131 - lr: 1.0000e-04\n",
      "Epoch 4/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.8825\n",
      "Epoch 4: val_accuracy improved from 0.91306 to 0.92986, saving model to best_model.h5\n",
      "981/981 [==============================] - 41s 41ms/step - loss: 0.2770 - accuracy: 0.8825 - val_loss: 0.1805 - val_accuracy: 0.9299 - lr: 1.0000e-04\n",
      "Epoch 5/25\n",
      "980/981 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9038\n",
      "Epoch 5: val_accuracy did not improve from 0.92986\n",
      "981/981 [==============================] - 41s 42ms/step - loss: 0.2367 - accuracy: 0.9039 - val_loss: 0.2057 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 6/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9185\n",
      "Epoch 6: val_accuracy improved from 0.92986 to 0.94931, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.2071 - accuracy: 0.9185 - val_loss: 0.1448 - val_accuracy: 0.9493 - lr: 1.0000e-04\n",
      "Epoch 7/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9246\n",
      "Epoch 7: val_accuracy did not improve from 0.94931\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.1929 - accuracy: 0.9246 - val_loss: 0.1536 - val_accuracy: 0.9475 - lr: 1.0000e-04\n",
      "Epoch 8/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9337\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.94931\n",
      "981/981 [==============================] - 41s 42ms/step - loss: 0.1712 - accuracy: 0.9337 - val_loss: 0.1610 - val_accuracy: 0.9475 - lr: 1.0000e-04\n",
      "Epoch 9/25\n",
      "980/981 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9502\n",
      "Epoch 9: val_accuracy improved from 0.94931 to 0.96522, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.1356 - accuracy: 0.9502 - val_loss: 0.1053 - val_accuracy: 0.9652 - lr: 5.0000e-05\n",
      "Epoch 10/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9546\n",
      "Epoch 10: val_accuracy did not improve from 0.96522\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.1190 - accuracy: 0.9546 - val_loss: 0.1104 - val_accuracy: 0.9637 - lr: 5.0000e-05\n",
      "Epoch 11/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9571\n",
      "Epoch 11: val_accuracy improved from 0.96522 to 0.96729, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.1147 - accuracy: 0.9571 - val_loss: 0.1027 - val_accuracy: 0.9673 - lr: 5.0000e-05\n",
      "Epoch 12/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9581\n",
      "Epoch 12: val_accuracy did not improve from 0.96729\n",
      "981/981 [==============================] - 40s 40ms/step - loss: 0.1115 - accuracy: 0.9581 - val_loss: 0.1089 - val_accuracy: 0.9658 - lr: 5.0000e-05\n",
      "Epoch 13/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9634\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.96729\n",
      "981/981 [==============================] - 40s 40ms/step - loss: 0.0998 - accuracy: 0.9634 - val_loss: 0.1210 - val_accuracy: 0.9623 - lr: 5.0000e-05\n",
      "Epoch 14/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9658\n",
      "Epoch 14: val_accuracy improved from 0.96729 to 0.97023, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.0931 - accuracy: 0.9658 - val_loss: 0.1012 - val_accuracy: 0.9702 - lr: 2.5000e-05\n",
      "Epoch 15/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9675\n",
      "Epoch 15: val_accuracy did not improve from 0.97023\n",
      "981/981 [==============================] - 40s 40ms/step - loss: 0.0874 - accuracy: 0.9675 - val_loss: 0.0974 - val_accuracy: 0.9679 - lr: 2.5000e-05\n",
      "Epoch 16/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9726\n",
      "Epoch 16: val_accuracy improved from 0.97023 to 0.97200, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.0761 - accuracy: 0.9726 - val_loss: 0.0891 - val_accuracy: 0.9720 - lr: 2.5000e-05\n",
      "Epoch 17/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9727\n",
      "Epoch 17: val_accuracy improved from 0.97200 to 0.97347, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.0741 - accuracy: 0.9727 - val_loss: 0.0935 - val_accuracy: 0.9735 - lr: 2.5000e-05\n",
      "Epoch 18/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9756\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.97347\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.0696 - accuracy: 0.9756 - val_loss: 0.0992 - val_accuracy: 0.9735 - lr: 2.5000e-05\n",
      "Epoch 19/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9755\n",
      "Epoch 19: val_accuracy improved from 0.97347 to 0.97554, saving model to best_model.h5\n",
      "981/981 [==============================] - 40s 41ms/step - loss: 0.0694 - accuracy: 0.9755 - val_loss: 0.0936 - val_accuracy: 0.9755 - lr: 1.2500e-05\n",
      "Epoch 20/25\n",
      "981/981 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9762\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.97554\n",
      "981/981 [==============================] - 40s 40ms/step - loss: 0.0650 - accuracy: 0.9762 - val_loss: 0.0986 - val_accuracy: 0.9735 - lr: 1.2500e-05\n",
      "Epoch 20: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, \n",
    "                              min_lr=5e-6, verbose=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, \n",
    "                               restore_best_weights=False, verbose=1)\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', \n",
    "                             mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "model = model(0.1, 30)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=validation_dataset,\n",
    "            batch_size=13,\n",
    "            epochs=25,\n",
    "            callbacks=[reduce_lr, early_stopping, checkpoint],\n",
    "            verbose=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16996 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    # image_size=(224, 224),\n",
    "    image_size=(224, 224),\n",
    "    seed=50,\n",
    "    shuffle=False,\n",
    "    batch_size=13\n",
    ")\n",
    "\n",
    "processed_dataset = prepare_dataset(full_dataset)\n",
    "\n",
    "total_samples = tf.data.experimental.cardinality(processed_dataset).numpy()\n",
    "\n",
    "train_size = int(0.75 * total_samples)                 # 70% for training\n",
    "val_size   = int(0.2 * total_samples)                # 20% for validation\n",
    "test_size = total_samples - train_size - val_size     # 10% for testing\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset       = full_dataset.take(train_size)\n",
    "validation_dataset  = full_dataset.skip(train_size).take(val_size)\n",
    "test_dataset        = full_dataset.skip(train_size + val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we'll assess the precision, recall, and F-1 score which are part of our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 22ms/step - loss: 0.0490 - accuracy: 0.9859\n",
      "Test Accuracy: 0.9858823418617249\n",
      "66/66 [==============================] - 2s 16ms/step\n",
      "Precision: 1.0\n",
      "Recall: 0.9858823529411764\n",
      "F1 Score: 0.9928909952606635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xpert/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test the Precision, Recall and F1 Score.\n",
    "\n",
    "model.load_weights(\"best_model.h5\")\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Get the prediciton\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Calculate Precision, Recall and F1 Score\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, this is our demonstration on creating predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "# You can choose an image from the folder prediction_subset and get the prediction.\n",
    "\n",
    "import random\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Choose a image randomly from the dataset\n",
    "folder_path = '../prediction_subset'\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "\n",
    "if image_files:\n",
    "    img_path = os.path.join(folder_path, random.choice(image_files))\n",
    "else:\n",
    "    raise FileNotFoundError(\"No images found in the folder.\")\n",
    "\n",
    "# preprocess the image so that the model can use it\n",
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "model = load_model(\"best_model.h5\")\n",
    "\n",
    "# Prepare the image to be fed into the model for prediction\n",
    "processed_image = preprocess_image(img_path)\n",
    "predictions = model.predict(processed_image)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
